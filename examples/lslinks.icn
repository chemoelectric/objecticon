import
   http,
   lang,
   net,
   xml,
   ipl.scan,
   ipl.pdco,
   ipl.options,
   ipl.charset,
   xdg,
   io,
   util,
   ipl.printf

global opts, hc

class HttpRequestHelperImpl(HttpRequestHelper)
   private auth

   public override get_authentication(httpc, dom, realm)
      local t
      if t := \auth then {
         # Only succeed once, otherwise httpclient will loop forever retrying
         auth := &null
         return t
      }
   end

   public override modify_redirect(httpc, hreq, hresp)
   end

   public new()
      local u, p
      \opts["a"] ? {
         u := tab(upto(':')) | syserr("Bad -a option")
         move(1)
         p := tab(0)
         auth := Authentication(u, p)
      }
      return
   end
end

procedure main(args)
   local t, cookies
   opts := options(args, [Opt("a",string_with(':'), "USER:PASS#Specify authorization"),
                          Opt("e",string, "URL#Set referer header"),
                          Opt("agent",string, "Set user agent field"),
                          Opt("A",, "Print URLs of any scheme, rather than http, https and file"),
                          Opt("I",integer_range(1,16), "N#Assume data is encoded in ISO 8859-N"),
                          Opt("i",, "Search for IMG links"),
                          Opt("b",, "Search for IMG and A links"),
                          Opt("tee",string,"FILE#Redirect retrieved page to FILE"),
                          Opt("t",integer_range(1),"TIMEOUT#Timeout (in seconds)"),
                          Opt("r",integer_range(0), "RETRIES#Number of retries"),
                          Opt("k",string, "FILE#Load cookies from file"),
                          Opt("text",,"Do a search for tags in the text content of the parsed document"),
                          Opt("P",host_port_opt(),"SERVER:PORT#Proxy server and port"),
                          Opt("Pa",authentication_opt,"USER:PASS#Proxy authorization"),
                          Opt("P5",, "Use SOCKS5 rather than HTTP CONNECT as the proxy protocol")
                          ],
                          "Usage: lslinks [URL]... [OPTIONS]\N_
                           List links in URL(s)")

   hc := HttpClient()
   hc.set_timeout(\opts["t"] * 1000)
   hc.set_retries(\opts["r"])
   hc.set_user_agent(\opts["agent"])
   if hc.set_proxy(\opts["P"]) then {
      hc.set_proxy_auth(\opts["Pa"])
      if \opts["P5"] then
         hc.set_proxy_type("socks5")
   }
   cookies := \opts["k"] | FilePath(BaseDir.ensure_data_home()).child("oicookies").str() | stopf("Couldn't access data dir: %w")
   t := decode_from_file(cookies) | table()
   hc.set_cookies(t)

   every do_one(!args)

   hc.close()
end

procedure do_one(a)
   local url, hr, res, src, doc, rs, f, s, i

   a := ucs(a) | stop("Argument not UTF-8")
   url := URL(a).normal() | stopf("Invalid url: %s: %w", a)

   if f := Files.url_to_file(url) then
      src := Files.file_to_string(f) | stopf("Couldn't read file: %w")
   else {
      url.scheme == ("http" | "https") | stop("Unknown url scheme: ", url.scheme)
      src := use {
         rs := RamStream(),
         {
            hr := HttpRequest().
               set_url(url).
               set_output_stream(rs).
               set_helper(HttpRequestHelperImpl())
            hr.set_referer(\opts["e"])
            res := hc.retrieve(hr) | stopf("Couldn't get: %w")
            rs.str()
         }
      }
      url := res.url
   }

   # Optionally save page source
   if s := \opts["tee"] then
      Files.string_to_file(s, src) | stopf("Couldn't dump file to %s: %w", s)

   src := if i := \opts["I"] then
      ISO8859.to_ucs(src, i) | stop("Invalid ISO 8859 number")
   else
      Text.liberal_ucs(src)

   doc := HtmlParser().parse(src)
   if \opts["i"] then
      do_search(url, src, doc, "IMG", u"SRC", "")
   else if \opts["b"] then {
      do_search(url, src, doc, "IMG", u"SRC", "IMG:")
      do_search(url, src, doc, "A", u"HREF", "A:")
      do_search(url, src, doc, "AREA", u"HREF", "A:")
   } else {
      do_search(url, src, doc, "A", u"HREF", "")
      do_search(url, src, doc, "AREA", u"HREF", "")
   }
end

procedure print(pfx, s, base)
   local url
   url := URL(s) | base.get_relative(s)
   # Normalize to ensure correct % encoding for flowterm output (no errant quotes for instance).
   url := url.normal()
   \opts["A"] | (url.scheme == ("http" | "https" | "file")) | fail
   if FileStream.stdout.is_flowterm_tty() then
      write(pfx, Files.begin_link(url), url.str(), Files.end_link())
   else
      write(pfx, url.str())
end

procedure remove_entities(s)
   return HtmlParser().parse("<HTML><PLAINTEXT>" || s).
      get_root_element().search_tree("PLAINTEXT").get_string_content()
end

procedure do_search(url, src, doc, tag, attr, pfx)
   local base, n, s

   base := url
   if n := doc.get_root_element().search_tree("BASE") then
      base := URL(n.get_attribute(u"HREF"))

   every n := Seq{doc.get_root_element().traverse_all_preorder()} do {
      if is(n, Element) then {
         if n.get_name() == tag & s := n.get_attribute(attr) then
            print(pfx, s, base)
      } else if \opts["text"] &
                s := text(n) | (is(n, Comment) & n.comment)
      then {
         s ? {
            while tab(caseless_past("<" || tag || " ")) do {
               if tab(caseless_past(attr || "=\"")) & s := tab(upto('\"')) then
                  print(pfx, remove_entities(s), base)
            }
         }
      }
   }
end
